+++
date = "2017-08-20"
draft = false
tags = ["human-activity-recognition", "tensorflow", "convolution-neural-network"]
title = "Human Activity Recognition using Convolution Neural Network"
math = true
disableSummary = true
+++
In this post[^1], we will discuss how to build a architecture for Human Activity Recognition using Convolution Neural Network. [Link to code](https://github.com/gomahajan/har-actitracker)

<!--more--> 
 # Python Libraries Used
1) TensorFlow
2) Pandas
3) Numpy
4) Scipy
5) Matplotlib

# Dataset
We are going to build a architecture which recognizes human activities like walking, lying down, standing, sitting, jogging etc. based on accelerometer data. We will use the Actitracker dataset from [Wireless Sensor Data Mining Lab](http://www.cis.fordham.edu/wisdm/dataset.php). 

Data for some activities in the dataset:

![](/img/har-cnn-actitracker/walking.png)
![](/img/har-cnn-actitracker/lying-down.png)

# Architecture
 Our architecture consists of multiple Depthwise Convolution Neural Networks (D-CONV), Max POOL and RELU layers, followed by fully connected neural network (FC) with a softmax classifier to get the class probabilities.

 [D-CONV -> RELU] -> POOL -> [D-CONV -> RELU] -> FC

 If you haven't heard of Depthwise Convolution Neural Networks, they are similar to normal Convolution Neural Networks except the filters are applied to each channel separately. So, a Convolution Neural Network which has input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, out_channels] will output a tensor of shape [batch, out_height, out_width, out_channels]. A Depthwise Convolution Neural Network which has input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, channel_multiplier] will output a tensor of shape [batch, out_height, out_width, in_channels*channel_multiplier] as it will separately apply channel_multiplier number of filters to each in_channel.

# Building the architecture in TensorFlow
We create placeholder variables for our input and output values. In TensorFlow, using 'None' as a dimension allows the variable to take any number of rows.

```python
self.X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, in_channels])
self.Y = tf.placeholder(tf.float32, shape=[None, num_labels])
```
Next, we create a Depthwise Convolution Neural Network (D-CNN) which has input tensor of shape [batch, in_height, in_width, in_channels] and a filter tensor of shape [filter_height, filter_width, in_channels, channel_multiplier] and will output a tensor of shape [batch, out_height, out_width, in_channels*channel_multiplier].
```python
filter = weight_variable([1, filter_width, in_channels, channels_multiplier])
depthwise_conv2d = tf.nn.depthwise_conv2d(input, filter, [1, 1, 1, 1], padding='VALID')
```
We follow the D-CNN with a RELU layer and add a bias for each channel.
```python
biases = bias_variable([channels_multiplier * in_channels])
tf.nn.relu(tf.add(depthwise_conv2d, biases))
```
We then add a max POOL layer which performs the max pooling over the window of ksize=[1, 1, kernel_size, 1] and uses a stride of [1, 1, stride_size, 1].
Note: We are performing the max pooling and stride only in the dimension of input_width. We do not want to pool across the channels or batch entries as we want to keep them independent. Also, input_height is 1 for D-CNN.

```python
p = tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1],
                          strides=[1, 1, stride_size, 1], padding='VALID')
```

We then add another D-CNN with filter tensor of shape [1, filter_width_2, in_channels_2, channel_multiplier_2]. in_channels_2 is equal to channels_multiplier * in_channels since D-CNN multiplies the number of channels by the channels_multiplier and our pooling preserved the
channel size.
Note: We, however, have to be careful when choosing the filter_width_2 as it should not be more than POOL layer's output width.

```python 
assert filter_width_2 <= p.shape[2], "Filter width 2 should be less than input width 2"
c = apply_depthwise_conv(p, filter_width_2, channels_multiplier * in_channels, channels_multiplier_2)
```

We flatten the output from the second D-CNN and pass it into a neural network.

```python
shape = c.get_shape().as_list()
c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])

assert shape[3] == channels_multiplier * in_channels * channels_multiplier_2

f_weights_l1 = weight_variable([shape[1] * shape[2] * shape[3], num_hidden])
f_biases_l1 = bias_variable([num_hidden])
f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1), f_biases_l1))
```
We then add a softmax classifier which outputs the class probabilities and loss.
```python
out_weights = weight_variable([num_hidden, num_labels])
out_biases = bias_variable([num_labels])
self.y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)
self.loss = -tf.reduce_sum(self.Y * tf.log(self.y_))
```

# Preprocessing data
 Using pandas library, we can read the data from the WISDM dataset using column names as indices for the returned DataFrame. 

 ```python
column_names = ['user-id', 'activity', 'timestamp', 'x-axis', 'y-axis', 'z-axis']
data = pd.read_csv(file_path, header=None, names=column_names, comment=';')
 ```

 We then drop the rows which have 'nan' entries and reduce the size in case we want to use only a certain percentage of the dataset [^2].
 ```python
data = data.dropna(axis=0, how='any')
data = data[0:(usage * data.shape[0]) // 100]
 ```

 We normalize the data independently from accelerometer along the x-axis, y-axis and z-axis.
 ```python
 def normalize(dataset):
    mu = np.mean(dataset, axis=0)
    sigma = np.std(dataset, axis=0)
    return (dataset - mu) / sigma

 dataset['x-axis'] = math.normalize(dataset['x-axis'])
 dataset['y-axis'] = math.normalize(dataset['y-axis'])
 dataset['z-axis'] = math.normalize(dataset['z-axis'])
 ```

Next, we segment the data into [windows_size, in_channels] blocks and we choose the label as the most occurring label in the window.
```python
def create_segments(data, window_size=90):
    segments = np.empty((0, window_size, 3))
    labels = np.empty((0))
    for (start, end) in windows(data['timestamp'], window_size):
        x = data["x-axis"][start:end]
        y = data["y-axis"][start:end]
        z = data["z-axis"][start:end]
        if (len(data['timestamp'][start:end]) == window_size):
            segments = np.vstack([segments, np.dstack([x, y, z])])
            labels = np.append(labels, stats.mode(data["activity"][start:end])[0][0])
    return segments, labels

segments, labels = util.create_segments(dataset)
 ```

 We then one hot encode the labels and reshape the segments into [1, windows_size, in_channels] blocks.
 ```python
 labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)
 reshaped_segments = segments.reshape(len(segments), 1, 90, 3)
 ```

 We do a 70:30 split of the data into train:test sets.

 ```python 
 train_test_split = np.random.rand(len(reshaped_segments)) < 0.70
 train_x = reshaped_segments[train_test_split]
 train_y = labels[train_test_split]
 test_x = reshaped_segments[~train_test_split]
 test_y = labels[~train_test_split]
 ```


# Training and evaluating the model
We batch the training data into sets of 10 and use gradient descent for learning weights. 

```python
learning_rate = 0.0001
training_epochs = 8
batch_size = 10
total_batches = train_x.shape[0] // batch_size
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(self.loss)

correct_prediction = tf.equal(tf.argmax(self.y_, 1), tf.argmax(self.Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

cost_history = np.empty(shape=[1], dtype=float)
```

We create a TensorFlow session, initialize the variables and train our model.
We print the accuracy on the training set for each epoch.
```python
with tf.Session() as session:
    tf.initialize_all_variables().run()
    for epoch in range(training_epochs):
        for b in range(total_batches):
            offset = (b * batch_size) % (train_y.shape[0] - batch_size)
            batch_x = train_x[offset:(offset + batch_size), :, :, :]
            batch_y = train_y[offset:(offset + batch_size), :]
            _, loss_value = session.run([optimizer, self.loss], feed_dict={self.X: batch_x, self.Y: batch_y})
            cost_history = np.append(cost_history, loss_value)
        print("Epoch: ", epoch, " Training Loss: ", loss_value, " Training Accuracy: ",
              session.run(accuracy, feed_dict={self.X: train_x, self.Y: train_y}))
```
We then get our accuracy on the test set.
```python
print("Testing Accuracy:", session.run(accuracy, feed_dict={self.X: test_x, self.Y: test_y}))
```


 [^1]: Inspired from a similar [post](https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN) by Aaqib Saeed
 [^2]: Helps when you are testing your architecture and do not want the "immenseness" of your data to slow you.